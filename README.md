# DSKD

Repo for Paper "Dual-Space Knowledge Distillation for Large Language Models".

## Data
The data used in our work is available [here](https://drive.google.com/drive/folders/1ZUsNVgWevACV9D-AHVNi9C7PX_2itzb8?usp=sharing)
